{% extends 'base.html' %}


{% block content %}
<h1>{% block title %} Methodology {% endblock %}</h1>
<hr style="width:50%;text-align:left;border-width: 2px;margin-left:0"> 
    <body>
        <p style="padding-top: 100px; font-size: 25px;"> 
            <b>Scraping Finacial Metrics</b>
        </p>
    </body>
    <body>
        <style>
            p {
            padding-top: 10px;
            font-size: 17px;
            }
        
        </style>
        <p>
            This is where Maurice will write about how he scraped the financial metrics.
        </p>    
    </body>
    <body>
        <p style="padding-top: 100px; font-size: 25px;"> 
            <b>Scraping Tweets</b>
        </p>
    </body>
    <body>
        <style>
            p {
            padding-top: 10px;
            font-size: 17px;
            }
        
        </style>
        <p>
            yvonne
        </p>    
    </body>
    <body>
        <p style="padding-top: 100px; font-size: 25px;"> 
            <b>NLP</b>
        </p>
    </body>
    <body>
        <style>
            p {
            padding-top: 10px;
            font-size: 17px;
            }
        
        </style>
        <p>
            Nam
        </p>    
    </body>
    <body>
        <p style="padding-top: 100px; font-size: 25px;"> 
            <b>Other: Ben Rapkin</b>
        </p>
    </body>
    <body>
        <style>
            p {
            padding-top: 10px;
            font-size: 17px;
            }
        
        </style>
        <p>
            After scraping the financial data, it needed to be processed into a standardized and usable format. The initial layout of the data had six measurements as rows and the time/date as a column attribute, so using pandas, I transposed the matrices and combined all measurements taken on the same date into a single row. This resulted in a new layout with six measurements as columns and the dates as rows. I applied this transformation to all the relevant financial data and then merged the resulting data frames based on date. The resulting data is displayed on the "Financial Data" tabs of our website.  </p>    
        <p>
            The next step was preparing the data for training. To do this, I created a new data frame where each row consisted of eight rows from the financial data, resulting in dimensions of (n-7) x (8xm). This was done to give the algorithms more context when making predictions. Finally, since we were trying to predict the closing price of NVDA and the assumption was that the algorithms would be run in the morning after the market opened, I dropped the columns related to the current day's metrics (high, low, close, volume, adjvolume) as the models would not yet have access to these values.        </p>
        <p>
            The next step in preparing the data was normalization, which was achieved by dividing each value by the maximum value in its row, resulting in a range of [0,1]. This was done to prevent the algorithms from being biased towards larger values. The data was then split into training (67.5%), validation (22.5%), and test (10%) sets. The first two sets were shuffled to ensure that the algorithms were learning to predict from eight-day histories, as otherwise they might overly rely on rules for how the market behaved during the period contained in the validation data, which may not apply to other periods. Experimentally, we found that shuffling the data resulted in significantly higher accuracy. It is worth noting that the test data was not shuffled (although when it was, the algorithms performed significantly better), as in actual use of stock price predictors, the data is typically sequential. After training, the scores of the algorithms were recorded in a CSV file for later reference.
        </p>
        <p>
            Unfortunately, due to issues other encountered with NLP, the twitter data has not yet been made available for processing, so the current financial models cannot be compared to their humanistic counterparts. However, hopefully this will be implemented very soon!
        </p>
        <p>
            After all the data analysis was done, the time came to build a frontend so that people could see, interpret, and interact with the data. The framework I settled on was Flask as it relates very closely to what weâ€™ve been doing in class. Flask is made to use python and can be coordinated with an SQLite database. Before this project I had never done any web development, so this was an interesting first for me and took up a ton of time
        </p>
        </body>

{% endblock %}
